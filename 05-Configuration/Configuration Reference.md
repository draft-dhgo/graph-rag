---
title: GraphRAG êµ¬ì„± ë ˆí¼ëŸ°ìŠ¤
tags:
  - configuration
  - reference
  - settings
created: 2025-01-13
type: documentation
links:
  - [[Architecture Overview]]
  - [[Index Module]]
  - [[Query Module]]
  - [[Language Model Module]]
  - [[Storage Module]]
---

# GraphRAG êµ¬ì„± ë ˆí¼ëŸ°ìŠ¤

**êµ¬ì„± ë ˆí¼ëŸ°ìŠ¤** ë¬¸ì„œëŠ” GraphRAG ì‹œìŠ¤í…œì˜ ëª¨ë“  ì„¤ì • ì˜µì…˜, íŒŒë¼ë¯¸í„°, ê¸°ë³¸ê°’ì— ëŒ€í•œ í¬ê´„ì ì¸ ê°€ì´ë“œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ğŸ“‹ ê°œìš”

GraphRAGëŠ” YAML ê¸°ë°˜ êµ¬ì„± íŒŒì¼ì„ ì‚¬ìš©í•˜ì—¬ ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ê³¼ ì¿¼ë¦¬ ì‹œìŠ¤í…œì˜ ëª¨ë“  ì¸¡ë©´ì„ ì œì–´í•©ë‹ˆë‹¤. êµ¬ì„± ì‹œìŠ¤í…œì€ ë‹¤ìŒì„ ì§€ì›í•©ë‹ˆë‹¤:

- **ë‹¤ì¤‘ í™˜ê²½**: ê°œë°œ, ìŠ¤í…Œì´ì§•, í”„ë¡œë•ì…˜ í™˜ê²½
- **í™˜ê²½ ë³€ìˆ˜ ì¹˜í™˜**: ë³´ì•ˆ ë° ìœ ì—°ì„±
- **íƒ€ì… ì•ˆì „ì„±**: Pydantic ê¸°ë°˜ ê²€ì¦
- **CLI ì¬ì •ì˜**: ëŸ°íƒ€ì„ ì„¤ì • ë³€ê²½

## ğŸ—ï¸ êµ¬ì„± ì•„í‚¤í…ì²˜

### ê¸°ë³¸ êµ¬ì¡°

```yaml
# ê·¸ë˜í”„ì˜ ìµœìƒìœ„ ë ˆë²¨ êµ¬ì¡°
root_dir: ./output
models:
  chat: ...
  embed: ...
input: ...
chunks: ...
output: ...
```

### êµ¬ì„± ë¡œë”© ìˆœì„œ

1. **ê¸°ë³¸ê°’**: ë‚´ë¶€ ê¸°ë³¸ê°’ ë¡œë“œ
2. **êµ¬ì„± íŒŒì¼**: YAML/JSON íŒŒì¼ ë¡œë“œ
3. **í™˜ê²½ ë³€ìˆ˜**: `${ENV_VAR}` ì¹˜í™˜
4. **CLI ì¸ì**: ëª…ë ¹ì¤„ ì¬ì •ì˜

## ğŸ“ êµ¬ì„± ì„¹ì…˜

### 1. ëª¨ë¸ êµ¬ì„± (models)

ì–¸ì–´ ëª¨ë¸ê³¼ ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤.

#### Chat ëª¨ë¸

```yaml
models:
  chat:
    type: openai_chat # ë˜ëŠ” azure_openai_chat
    model: gpt-4-turbo-preview
    api_key: ${OPENAI_API_KEY}
    temperature: 0.0
    max_tokens: 2000
    # ì„±ëŠ¥ ì„¤ì •
    concurrent_requests: 10
    max_retries: 5
    request_timeout: 60.0
    # ì†ë„ ì œí•œ
    tokens_per_minute: auto
    requests_per_minute: auto
```

#### Embedding ëª¨ë¸

```yaml
models:
  embed:
    type: openai_embedding # ë˜ëŠ” azure_openai_embedding
    model: text-embedding-3-small
    api_key: ${OPENAI_API_KEY}
    # ë°°ì¹˜ ì²˜ë¦¬
    batch_size: 16
    concurrent_requests: 25
```

### 2. ì…ë ¥ êµ¬ì„± (input)

ë°ì´í„° ì†ŒìŠ¤ì™€ í˜•ì‹ì„ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
input:
  type: file # file, blob
  file_type: text # text, csv, json
  encoding: utf-8
  file_pattern: "input/*.txt"
  # CSV íŠ¹ì •
  text_column: text
  title_column: title
  # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ
  metadata: []
```

### 3. ì²­í‚¹ êµ¬ì„± (chunks)

í…ìŠ¤íŠ¸ ë¶„í•  ì „ëµì„ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
chunks:
  size: 1200 # ì²­í¬ í¬ê¸° (í† í°)
  overlap: 100 # ì²­í¬ ê°„ ì¤‘ë³µ
  strategy: tokens # tokens, sentences
```

### 4. ì¶œë ¥ êµ¬ì„± (output)

ê²°ê³¼ ì €ì¥ ìœ„ì¹˜ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
output:
  type: file # file, blob, cosmosdb
  base_dir: ./output
```

### 5. ê·¸ë˜í”„ ì¶”ì¶œ êµ¬ì„± (extract_graph)

ì—”í‹°í‹°ì™€ ê´€ê³„ ì¶”ì¶œì„ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
extract_graph:
  prompt: null # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©
  entity_types: # ì¶”ì¶œí•  ì—”í‹°í‹° ìœ í˜•
    - organization
    - person
    - location
    - event
    - gpe
  max_gleanings: 1 # ì¬ì¶”ì¶œ íšŸìˆ˜
  # LLM ì„¤ì •
  model: chat
  tuple_delimiter: "<|>"
  record_delimiter: "##"
  completion_delimiter: "<|COMPLETE|>"
```

### 6. ì»¤ë®¤ë‹ˆí‹° êµ¬ì„± (cluster_graph)

ì»¤ë®¤ë‹ˆí‹° íƒì§€ íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
cluster_graph:
  # Leiden ì•Œê³ ë¦¬ì¦˜
  max_cluster_size: 50
  hierarchy_level: null # null = ìë™ ì„ íƒ
  use_lcc: true # ìµœëŒ€ ì—°ê²° ì»´í¬ë„ŒíŠ¸ ì‚¬ìš©
  # Leiden íŒŒë¼ë¯¸í„°
  resolution: 1.0
```

### 7. ì»¤ë®¤ë‹ˆí‹° ë¦¬í¬íŠ¸ êµ¬ì„± (community_reports)

ì»¤ë®¤ë‹ˆí‹° ìš”ì•½ ìƒì„±ì„ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
community_reports:
  prompt: null
  model: chat
  max_length: 2000
  max_input_length: 8000
  # ìƒì„± íŒŒë¼ë¯¸í„°
  temperature: 0.0
  top_p: 1.0
  n: 1
```

### 8. ì„ë² ë”© êµ¬ì„± (embed_graph)

ê·¸ë˜í”„ ìš”ì†Œì˜ ì„ë² ë”©ì„ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
embed_graph:
  model: embed
  # ë¬´ì—‡ì„ ì„ë² ë”©í• ì§€
  embed_graph_embedding: true # ë…¸ë“œ ì„ë² ë”©
  embed_graph_attributes: false # ì†ì„± ì„ë² ë”©
  embed_graph_knowledge: true # ì„¤ëª… ì„ë² ë”©
  # ë²¡í„° ìŠ¤í† ì–´
  vector_store: lancedb
```

### 9. ë¡œì»¬ ê²€ìƒ‰ êµ¬ì„± (local_search)

```yaml
local_search:
  # í…ìŠ¤íŠ¸ ìœ ë‹›
  text_unit_prop: 0.5 # í…ìŠ¤íŠ¸ ìœ ë‹› ë¹„ìœ¨
  # ì»¤ë®¤ë‹ˆí‹°
  community_prop: 0.1 # ì»¤ë®¤ë‹ˆí‹° ë¹„ìœ¨
  # conversations
  conversation_history_max_turns: 5
  # ì»¨í…ìŠ¤íŠ¸
  top_k_mapped_entities: 10
  top_k_relationships: 10
  max_context_tokens: 8000
  # LLM
  model: chat
  temperature: 0.0
  top_p: 1.0
  n: 1
```

### 10. ê¸€ë¡œë²Œ ê²€ìƒ‰ êµ¬ì„± (global_search)

```yaml
global_search:
  # ì»¤ë®¤ë‹ˆí‹°
  max_level: 3 # ê³„ì¸µ ë ˆë²¨
  use_community_summary: true
  # ì»¨í…ìŠ¤íŠ¸
  max_context_tokens: 12000
  token_budget: 4000
  # LLM
  model: chat
  temperature: 0.0
  top_p: 1.0
  n: 1
  # ì •ë ¬
  rank_conversation_history: false
```

### 11. UMAP êµ¬ì„± (umap)

ì°¨ì› ì¶•ì†Œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.

```yaml
umap:
  num_neighbors: 15
  min_dist: 0.1
  metric: cosine
```

### 12. ìºì‹œ êµ¬ì„± (cache)

```yaml
cache:
  type: file # file, memory, blob, cosmosdb
  base_dir: ./cache
```

### 13. ë¦¬í¬íŒ… êµ¬ì„± (reporting)

```yaml
reporting:
  type: file # file, blob, console
  base_dir: ./reports
```

## ğŸ”§ í™˜ê²½ ë³€ìˆ˜

### ì§€ì›ë˜ëŠ” í™˜ê²½ ë³€ìˆ˜

| ë³€ìˆ˜ | ì„¤ëª… | ì˜ˆì‹œ |
|------|------|------|
| `OPENAI_API_KEY` | OpenAI API í‚¤ | `sk-...` |
| `AZURE_OPENAI_API_KEY` | Azure OpenAI í‚¤ | `...` |
| `AZURE_OPENAI_ENDPOINT` | Azure OpenAI ì—”ë“œí¬ì¸íŠ¸ | `https://...` |
| `AZURE_OPENAI_API_VERSION` | Azure API ë²„ì „ | `2024-02-01` |
| `GRAPHRAG_ROOT_DIR` | ê¸°ë³¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ | `./output` |
| `GRAPHRAG_CACHE_DIR` | ìºì‹œ ë””ë ‰í† ë¦¬ | `./cache` |

### í™˜ê²½ ë³€ìˆ˜ ì‚¬ìš©

```yaml
models:
  chat:
    api_key: ${OPENAI_API_KEY}
    api_base: ${AZURE_OPENAI_ENDPOINT:-https://api.openai.com/v1}
```

## ğŸ“Š ì „ì²´ ì˜ˆì œ êµ¬ì„±

```yaml
# GraphRAG êµ¬ì„± íŒŒì¼ ì˜ˆì œ
---
# ê¸°ë³¸ ê²½ë¡œ
root_dir: ./output

# ì–¸ì–´ ëª¨ë¸
models:
  chat:
    type: openai_chat
    model: gpt-4-turbo-preview
    api_key: ${OPENAI_API_KEY}
    temperature: 0.0
    max_tokens: 2000
    concurrent_requests: 10
    max_retries: 5
    request_timeout: 60.0

  embed:
    type: openai_embedding
    model: text-embedding-3-small
    api_key: ${OPENAI_API_KEY}
    batch_size: 16
    concurrent_requests: 25

# ì…ë ¥
input:
  type: file
  file_type: text
  encoding: utf-8
  file_pattern: "input/*.txt"

# ì²­í‚¹
chunks:
  size: 1200
  overlap: 100
  strategy: tokens

# ì¶œë ¥
output:
  type: file
  base_dir: ./output

# ê·¸ë˜í”„ ì¶”ì¶œ
extract_graph:
  model: chat
  entity_types:
    - organization
    - person
    - location
    - event
    - gpe
  max_gleanings: 1

# ì»¤ë®¤ë‹ˆí‹° íƒì§€
cluster_graph:
  max_cluster_size: 50
  use_lcc: true

# ì»¤ë®¤ë‹ˆí‹° ë¦¬í¬íŠ¸
community_reports:
  model: chat
  max_length: 2000
  max_input_length: 8000

# ì„ë² ë”©
embed_graph:
  model: embed
  embed_graph_embedding: true

# ë¡œì»¬ ê²€ìƒ‰
local_search:
  text_unit_prop: 0.5
  community_prop: 0.1
  top_k_mapped_entities: 10
  top_k_relationships: 10
  max_context_tokens: 8000

# ê¸€ë¡œë²Œ ê²€ìƒ‰
global_search:
  max_level: 3
  use_community_summary: true
  max_context_tokens: 12000

# ìºì‹œ
cache:
  type: file
  base_dir: ./cache

# ë¦¬í¬íŒ…
reporting:
  type: file
  base_dir: ./reports
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. ìƒˆ êµ¬ì„± íŒŒì¼ ìƒì„±

```bash
graphrag init --root ./my-project
```

### 2. êµ¬ì„± íŒŒì¼ ì‚¬ìš©

```bash
# ê¸°ë³¸ êµ¬ì„±
graphrag index --root ./my-project

# ì‚¬ìš©ì ì •ì˜ êµ¬ì„±
graphrag index --config ./custom-config.yaml

# CLI ì¬ì •ì˜
graphrag index --root . --input.file_pattern "data/*.txt"
```

### 3. êµ¬ì„± ê²€ì¦

```bash
# êµ¬ì„± ìœ íš¨ì„± ê²€ì‚¬
graphrag init --validate --config ./settings.yaml
```

## ğŸ“– ê´€ë ¨ ë¬¸ì„œ

- [[Index Module]] - ì¸ë±ì‹± íŒŒì´í”„ë¼ì¸ êµ¬ì„±
- [[Query Module]] - ì¿¼ë¦¬ ì‹œìŠ¤í…œ êµ¬ì„±
- [[Language Model Module]] - LLM ì„¤ì • ìƒì„¸
- [[Storage Module]] - ìŠ¤í† ë¦¬ì§€ ì˜µì…˜

---

*ë§ˆì§€ë§‰ ì—…ë°ì´íŠ¸: 2025-01-13*
