---
title: LLM Integration Deep Dive
tags:
  - deep-dive
  - llm
  - openai
  - azure
  - rate-limiting
  - implementation
created: 2025-01-12
type: deep-dive
links:
  - [[Language Model Module]]
  - [[Index Module]]
  - [[Query Module]]
---

# LLM Integration Deep Dive

GraphRAGì˜ LLM í†µí•© ê³„ì¸µì€ OpenAI, Azure OpenAI, LiteLLM ë“± ë‹¤ì–‘í•œ LLM ì œê³µìì™€ì˜ í†µí•©ì„ ë‹´ë‹¹í•˜ë©°, ì†ë„ ì œí•œ, ì¬ì‹œë„, í† í° ê´€ë¦¬ë¥¼ í¬í•¨í•œ í”„ë¡œë•ì…˜ ì¤€ë¹„ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.

## ëª©ì°¨

### 1. ê°œìš”
- [LLM ëª¨ë“ˆì˜ ì±…ì„](#-llm-ëª¨ë“ˆì˜-ì±…ì„)
- [ë¹—ëŒ€ì–´ ë³´ê¸°: ê³ ì†ë„ë¡œ í†¨ê²Œì´íŠ¸ ì‹œìŠ¤í…œ](#-ë¹—ëŒ€ì–´-ë³´ê¸°-ê³ ì†ë„ë¡œ-í†¨ê²Œì´íŠ¸-ì‹œìŠ¤í…œ)

### 2. ì•„í‚¤í…ì²˜
- [ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜](#-ì•„í‚¤í…ì²˜)
- [ê³„ì¸µë³„ ìƒì„¸](#-ê³„ì¸µë³„-ìƒì„¸)

### 3. ì œê³µìë³„ êµ¬í˜„
- [OpenAI](#1-openai)
- [Azure OpenAI](#2-azure-openai)
- [LiteLLM](#3-litellm)

### 4. ì†ë„ ì œí•œ ê´€ë¦¬
- [Rate Limiter êµ¬í˜„](âš¡-ì†ë„-ì œí•œ-rate-limiting)
- [ì´ì¤‘ ì œí•œ ì‹œìŠ¤í…œ](#ì´ì¤‘-ì œí•œ-ì‹œìŠ¤í…œ)
- [ëŒ€ê¸° ì „ëµ](#ëŒ€ê¸°-ì „ëµ)

### 5. ì¬ì‹œë„ ë¡œì§
- [ì§€ìˆ˜ ë°±ì˜¤í”„](#ì§€ìˆ˜-ë°±ì˜¤í”„-exponential-backoff)
- [ì¬ì‹œë„ ì „ëµ ë¹„êµ](#ì¬ì‹œë„-ì „ëµ-ë¹„êµ)

### 6. í† í° ê´€ë¦¬
- [í† í° ì¹´ìš´í„°](#-í† í°-ê´€ë¦¬)
- [ëª¨ë¸ë³„ ì¸ì½”ë”©](#ëª¨ë¸ë³„-ì¸ì½”ë”©)

### 7. ê³ ê¸‰ ê¸°ë²•
- [ë¹„ë™ê¸° ì²˜ë¦¬](#-ë¹„ë™ê¸°-ì²˜ë¦¬)
- [ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ](#ìŠ¤íŠ¸ë¦¬ë°-ì‘ë‹µ)
- [í•¨ìˆ˜ í˜¸ì¶œ](#í•¨ìˆ˜ í˜¸ì¶œ-function-calling)

---

## ğŸ¯ LLM ëª¨ë“ˆì˜ ì±…ì„

```mermaid
flowchart LR
    A[LLM ëª¨ë“ˆ] --> B[ë‹¤ì¤‘ ì œê³µì ì§€ì›]
    A --> C[ì†ë„ ì œí•œ ê´€ë¦¬]
    A --> D[ì¬ì‹œë„ ë¡œì§]
    A --> E[ë¹„ë™ê¸° ì²˜ë¦¬]
    A --> F[í† í° ì¶”ì ]

    style A fill:#fff9c4
    style B fill:#c8e6c9
    style C fill:#e1bee7
    style D fill:#ffccbc
    style E fill:#fff59d
    style F fill:#e1f5fe
```

1. **ë‹¤ì¤‘ ì œê³µì ì§€ì›**: OpenAI, Azure OpenAI, 100+ LiteLLM ì œê³µì
2. **ì†ë„ ì œí•œ ê´€ë¦¬**: RPM/TPM ì´ì¤‘ ì œí•œ
3. **ì¬ì‹œë„ ë¡œì§**: ì§€ìˆ˜ ë°±ì˜¤í”„ ë° ì ì§„ì  ì§€ì—°
4. **ë¹„ë™ê¸° ì²˜ë¦¬**: ë³‘ë ¬ ìš”ì²­ ì²˜ë¦¬
5. **í† í° ì¶”ì **: ì‚¬ìš©ëŸ‰ ëª¨ë‹ˆí„°ë§

## ğŸ“– ë¹—ëŒ€ì–´ ë³´ê¸°: ê³ ì†ë„ë¡œ í†¨ê²Œì´íŠ¸ ì‹œìŠ¤í…œ

LLM í†µí•©ì˜ ì†ë„ ì œí•œ ê´€ë¦¬ëŠ” **ê³ ì†ë„ë¡œ í†¨ê²Œì´íŠ¸ì˜ ì°¨ëŸ‰ í†µì œ ì‹œìŠ¤í…œ**ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤:

| í†¨ê²Œì´íŠ¸ ì‹œìŠ¤í…œ | LLM ì†ë„ ì œí•œ |
|---------------|--------------|
| ì°¨ëŸ‰ ì§„ì… | API ìš”ì²­ |
| ì‹œê°„ë‹¹ ì°¨ëŸ‰ ì œí•œ | RPM (Requests Per Minute) |
| í•˜ì¤‘ ì œí•œ | TPM (Tokens Per Minute) |
| ëŒ€ê¸° í | ìš”ì²­ ëŒ€ê¸°ì—´ |
| ìš°ì„  ì°¨ëŸ‰ | ìš°ì„ ìˆœìœ„ ì²˜ë¦¬ |
| ì¬ì§„ì… ëŒ€ê¸° ì‹œê°„ | ì¬ì‹œë„ ì§€ì—° |

```mermaid
flowchart TB
    subgraph Toll["í†¨ê²Œì´íŠ¸ ì‹œìŠ¤í…œ"]
        T1["ğŸš— ì°¨ëŸ‰ ë„ì°©"] --> T2["ğŸ« í†¨ê²Œì´íŠ¸"]
        T2 --> T3{"ì œí•œ í™•ì¸"}
        T3 -->|ì´ˆê³¼| T4["â³ ëŒ€ê¸°"]
        T3 -->|ê°€ëŠ¥| T5["âœ… í†µê³¼"]
        T4 --> T3
    end

    subgraph LLM["LLM ì†ë„ ì œí•œ"]
        L1["ğŸ“¡ API ìš”ì²­"] --> L2["ğŸ”’ Rate Limiter"]
        L2 --> L3{"ì œí•œ í™•ì¸"}
        L3 -->|ì´ˆê³¼| L4["â³ ëŒ€ê¸°"]
        L3 -->|ê°€ëŠ¥| L5["âœ… LLM í˜¸ì¶œ"]
        L4 --> L3
    end

    style Toll fill:#e3f2fd
    style LLM fill:#f3e5f5
```

## ğŸ—ï¸ ì•„í‚¤í…ì²˜

```mermaid
flowchart TB
    subgraph Client["í´ë¼ì´ì–¸íŠ¸ ì½”ë“œ"]
        API["build_index<br/>local_search<br/>global_search"]
    end

    subgraph Factory["LLM Factory"]
        OF["OpenAI<br/>Factory"]
        AF["Azure OpenAI<br/>Factory"]
        LF["LiteLLM<br/>Factory"]
    end

    subgraph Models["LLM ì¸ìŠ¤í„´ìŠ¤"]
        OAI["OpenAI Chat<br/>gpt-4-turbo"]
        AOA["Azure OpenAI Chat<br/>gpt-4"]
        LLM["LiteLLM<br/>claude-3-opus"]
    end

    subgraph Protection["ë³´í˜¸ ê³„ì¸µ"]
        RL["Rate Limiter<br/>ğŸ¯ RPM + TPM"]
        RT["Retry Handler<br/>ğŸ”„ Exponential Backoff"]
        TOK["Token Counter<br/>ğŸ“Š tiktoken"]
    end

    API --> Factory
    Factory --> OAI
    Factory --> AOA
    Factory --> LLM

    OAI --> RL --> RT
    AOA --> RL --> RT
    LLM --> RL --> RT

    RT --> TOK
    TOK --> RESP["LLM Response<br/>âœ… ê²°ê³¼ ë°˜í™˜"]

    style Client fill:#e3f2fd,stroke:#1976d2,stroke-width:2px
    style Factory fill:#fff3e0,stroke:#f57c00,stroke-width:2px
    style Models fill:#e8f5e9,stroke:#388e3c,stroke-width:2px
    style Protection fill:#fce4ec,stroke:#c2185b,stroke-width:2px
    style RESP fill:#c8e6c9,stroke:#2e7d32,stroke-width:3px
```

### ê³„ì¸µë³„ ìƒì„¸

```mermaid
flowchart TB
    subgraph L1["ê³„ì¸µ 1: íŒ©í† ë¦¬"]
        F1["ì œê³µì ì„ íƒ<br/>ì„¤ì • ë¡œë“œ"]
    end

    subgraph L2["ê³„ì¸µ 2: ëª¨ë¸"]
        M1["LLM ì¸ìŠ¤í„´ìŠ¤í™”<br/>ëª¨ë¸ ì„¤ì •"]
    end

    subgraph L3["ê³„ì¸µ 3: ë³´í˜¸"]
        P1["ì†ë„ ì œí•œ<br/>ì¬ì‹œë„ ë¡œì§<br/>í† í° ì¹´ìš´íŒ…"]
    end

    subgraph L4["ê³„ì¸µ 4: ì „ì†¡"]
        T1["HTTP ìš”ì²­<br/>ì‘ë‹µ íŒŒì‹±"]
    end

    F1 --> M1 --> P3 --> T1

    style L1 fill:#e1f5fe
    style L2 fill:#fff9c4
    style L3 fill:#ffcdd2
    style L4 fill:#c8e6c9
```

## ğŸ“‹ ì œê³µìë³„ êµ¬í˜„

### 1. OpenAI

```mermaid
flowchart TB
    A["OpenAI Chat"] --> B["API Key ì¸ì¦"]
    B --> C["gpt-4-turbo-preview<br/>gpt-4o<br/>gpt-3.5-turbo"]

    C --> D["StaticRateLimiter<br/>RPM: 500<br/>TPM: 100,000"]

    D --> E["RetryConfig<br/>max_retries: 10<br/>max_wait: 60s"]

    style A fill:#c8e6c9
    style C fill:#e1f5fe
    style D fill:#fff9c4
    style E fill:#e1bee7
```

```python
class OpenAIChat(BaseLanguageModel):
    def __init__(
        self,
        model: str = "gpt-4-turbo-preview",
        api_key: str | None = None,
        temperature: float = 0.0,
        max_tokens: int = 4000,
        requests_per_minute: int = 500,
        tokens_per_minute: int = 100000,
    ):
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
        self.rate_limiter = StaticRateLimiter(
            requests_per_minute,
            tokens_per_minute
        )
        self.retry_config = RetryConfig(
            strategy="exponential_backoff",
            max_retries=10,
            max_retry_wait=60
        )
```

### 2. Azure OpenAI

```mermaid
flowchart TB
    A["Azure OpenAI Chat"] --> B1["API Key ì¸ì¦"]
    A --> B2["Managed Identity"]

    B1 --> C["Azure Endpoint<br/>API Version<br/>Deployment Name"]

    C --> D["Azure Credential"]

    style A fill:#fff9c4
    style B1 fill:#c8e6c9
    style B2 fill:#e1bee7
    style D fill:#e1f5fe
```

```python
class AzureOpenAIChat(BaseLanguageModel):
    def __init__(
        self,
        model: str,
        api_base: str,
        api_version: str,
        deployment_name: str,
        api_key: str | None = None,
        auth_type: str = "api_key",  # or "azure_managed_identity"
    ):
        if auth_type == "azure_managed_identity":
            # DefaultAzureCredential ì‚¬ìš©
            from azure.identity import DefaultAzureCredential
            credential = DefaultAzureCredential()

            self.client = AsyncAzureOpenAI(
                azure_endpoint=api_base,
                api_version=api_version,
                azure_deployment=deployment_name,
                credential=credential
            )
        else:
            self.client = AsyncAzureOpenAI(
                azure_endpoint=api_base,
                api_version=api_version,
                azure_deployment=deployment_name,
                api_key=api_key
            )
```

### 3. LiteLLM

```mermaid
flowchart TB
    A["LiteLLM"] --> B1["Claude"]
    A --> B2["Gemini"]
    A --> B3["HuggingFace"]
    A --> B4["100+ ì œê³µì"]

    B1 --> C["í†µí•© ì¸í„°í˜ì´ìŠ¤"]
    B2 --> C
    B3 --> C
    B4 --> C

    style A fill:#e1bee7
    style C fill:#c8e6c9
```

```python
class LiteLLMModel(BaseLanguageModel):
    def __init__(
        self,
        model: str,  # e.g., "claude-3-opus-20240229"
        model_provider: str,  # e.g., "anthropic"
        api_key: str | None = None,
    ):
        import litellm

        self.model = model
        self.model_provider = model_provider
        self.litellm_client = litellm.aasync_litellm
```

## âš¡ ì†ë„ ì œí•œ (Rate Limiting)

### Rate Limiter êµ¬í˜„

```mermaid
flowchart TB
    START([ìš”ì²­ ë„ì°©]) --> CHECK_RPM{RPM<br/>í™•ì¸}

    CHECK_RPM -->|ì´ˆê³¼| WAIT_RPM["â³ ëŒ€ê¸°<br/>ê°€ì¥ ì˜¤ë˜ëœ ìš”ì²­<br/>1ë¶„ ê²½ê³¼ ëŒ€ê¸°"]
    CHECK_RPM -->|ê°€ëŠ¥| CHECK_TPM{TPM<br/>í™•ì¸}

    WAIT_RPM --> CHECK_RPM

    CHECK_TPM -->|ì´ˆê³¼| WAIT_TPM["â³ ëŒ€ê¸°<br/>í† í° í™•ë³´<br/>ì‹œê°„ ê³„ì‚°"]
    CHECK_TPM -->|ê°€ëŠ¥| ACQUIRE["âœ… ìš”ì²­ í—ˆìš©"]

    WAIT_TPM --> CHECK_TPM

    ACQUIRE --> PROCESS[LLM ì²˜ë¦¬]

    PROCESS --> UPDATE["ğŸ“Š ê¸°ë¡ ì—…ë°ì´íŠ¸<br/>ìš”ì²­ ì‹œê°„<br/>í† í° ìˆ˜"]

    UPDATE --> END([ì™„ë£Œ])

    style START fill:#e1f5fe
    style END fill:#c8e6c9
    style WAIT_RPM fill:#ffcdd2
    style WAIT_TPM fill:#ffcdd2
    style ACQUIRE fill:#a5d6a7
```

```python
import asyncio
import time
from collections import deque
from typing import Deque

class StaticRateLimiter:
    def __init__(
        self,
        requests_per_minute: int,
        tokens_per_minute: int
    ):
        self.rpm = requests_per_minute
        self.tpm = tokens_per_minute

        # ìš”ì²­ ì´ë ¥ (ì‹œê°„ ìŠ¤íƒ¬í”„)
        self.request_history: Deque[float] = deque()

        # í† í° ì´ë ¥ (ì‹œê°„, í† í° ìˆ˜) ìŒ
        self.token_history: Deque[tuple[float, int]] = deque()

    async def acquire(self, tokens: int = 0):
        """
        ì†ë„ ì œí•œ ì¤€ìˆ˜ê¹Œì§€ ëŒ€ê¸°
        """
        now = time.time()

        # 1. ìš”ì²­ ì œí•œ í™•ì¸
        while len(self.request_history) >= self.rpm:
            # ê°€ì¥ ì˜¤ë˜ëœ ìš”ì²­ì´ 1ë¶„ ì´ìƒ ì „ì¸ì§€ í™•ì¸
            oldest_request = self.request_history[0]
            if now - oldest_request >= 60:
                self.request_history.popleft()
            else:
                # ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
                wait_time = 60 - (now - oldest_request)
                await asyncio.sleep(wait_time)
                now = time.time()

        # 2. í† í° ì œí•œ í™•ì¸
        recent_tokens = sum(
            count for ts, count in self.token_history
            if now - ts < 60
        )

        if recent_tokens + tokens > self.tpm:
            # ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
            available_in = self._get_time_for_tokens(tokens)
            await asyncio.sleep(available_in)
            now = time.time()

        # 3. ê¸°ë¡
        self.request_history.append(now)
        self.token_history.append((now, tokens))
```

### ì´ì¤‘ ì œí•œ ì‹œìŠ¤í…œ

```mermaid
flowchart TB
    A["ìš”ì²­"] --> RPM{"RPM ì œí•œ<br/>500/ë¶„"}
    A --> TPM{"TPM ì œí•œ<br/>100K/ë¶„"}

    RPM -->|ì´ˆê³¼| RPMBLOCK["ìš”ì²­ ëŒ€ê¸°"]
    TPM -->|ì´ˆê³¼| TPMBLOCK["í† í° ëŒ€ê¸°"]

    RPMBLOCK --> RPM
    TPMBLOCK --> TPM

    RPM -->|í†µê³¼| BOTH["ë‘ ì œí•œ ëª¨ë‘<br/>í†µê³¼ í•„ìš”"]
    TPM -->|í†µê³¼| BOTH

    BOTH --> EXEC["LLM ì‹¤í–‰"]

    style RPM fill:#fff9c4
    style TPM fill:#e1bee7
    style RPMBLOCK fill:#ffcdd2
    style TPMBLOCK fill:#ffcdd2
    style EXEC fill:#c8e6c9
```

| ì œí•œ ìœ í˜• | ëª©ì  | ì¼ë°˜ì  ê°’ |
|----------|------|-----------|
| **RPM** | ìš”ì²­ ë¹ˆë„ ì œí•œ | 500/ë¶„ |
| **TPM** | í† í° ì²˜ë¦¬ëŸ‰ ì œí•œ | 100,000/ë¶„ |
| **TPD** | ì¼ì¼ í† í° ì œí•œ | 10,000,000/ì¼ |

### ëŒ€ê¸° ì „ëµ

```mermaid
flowchart TB
    A["ì†ë„ ì œí•œ ë„ë‹¬"] --> B{ì „ëµ ì„ íƒ}

    B -->|ê³ ì •| F["ê³ ì • ëŒ€ê¸°<br/>â¸ï¸ 60ì´ˆ"]
    B -->|ì„ í˜•| L["ì„ í˜• ëŒ€ê¸°<br/>ğŸ“ˆ 10s Ã— ì¬ì‹œë„"]
    B -->|ì§€ìˆ˜| E["ì§€ìˆ˜ ëŒ€ê¸°<br/>ğŸ“ˆ 2^ì¬ì‹œë„"]
    B -->|ì§€í„°| J["ì§€í„° ì¶”ê°€<br/>ğŸ² ë¬´ì‘ìœ„ì„±"]

    F --> NEXT["ë‹¤ìŒ ì‹œë„"]
    L --> NEXT
    E --> NEXT
    J --> NEXT

    style A fill:#ffcdd2
    style E fill:#c8e6c9
    style F fill:#fff9c4
    style L fill:#e1bee7
    style J fill:#e1f5fe
```

## ğŸ”„ ì¬ì‹œë„ ë¡œì§ (Retry Logic)

### ì§€ìˆ˜ ë°±ì˜¤í”„ (Exponential Backoff)

```mermaid
flowchart TB
    A["API í˜¸ì¶œ"] --> B{ì„±ê³µ?}

    B -->|ì˜ˆ| SUCCESS["âœ… ê²°ê³¼ ë°˜í™˜"]
    B -->|ì‹¤íŒ¨| ERROR{ì—ëŸ¬ ìœ í˜•}

    ERROR -->|RateLimit| RETRY["ì¬ì‹œë„ ê²°ì •"]
    ERROR -->|Timeout| RETRY
    ERROR -->|ê¸°íƒ€| FAIL["âŒ ì‹¤íŒ¨"]

    RETRY --> C{ì¬ì‹œë„<br/>íšŸìˆ˜ í™•ì¸}

    C -->|ìµœëŒ€ ë„ë‹¬| FAIL
    C -->|ê°€ëŠ¥| D["ëŒ€ê¸° ì‹œê°„ ê³„ì‚°<br/>2^attempt Ã— base"]

    D --> E["ì§€í„° ì¶”ê°€<br/>Ã— (0.5~1.0)"]

    E --> WAIT["â³ ëŒ€ê¸°"]

    WAIT --> A

    style SUCCESS fill:#c8e6c9
    style FAIL fill:#ffcdd2
    style RETRY fill:#fff9c4
    style WAIT fill:#e1bee7
```

```python
async def execute_with_retry(
    llm_call: Callable,
    max_retries: int = 10,
    base_delay: float = 1.0,
    max_delay: float = 60.0,
    jitter: bool = True
):
    """
    ì§€ìˆ˜ ë°±ì˜¤í”„ë¥¼ ì‚¬ìš©í•œ ì¬ì‹œë„ ì‹¤í–‰
    """
    last_error = None

    for attempt in range(max_retries + 1):
        try:
            return await llm_call()

        except RateLimitError as e:
            if attempt == max_retries:
                raise MaxRetriesError(f"Max retries exceeded: {e}")

            # ëŒ€ê¸° ì‹œê°„ ê³„ì‚°
            delay = min(base_delay * (2 ** attempt), max_delay)

            # Jitter ì¶”ê°€ (ë¶ˆëŒ ë°©ì§€)
            if jitter:
                import random
                delay = delay * (0.5 + random.random())

            logging.warning(f"Rate limited. Retry {attempt + 1}/{max_retries} after {delay:.2f}s")
            await asyncio.sleep(delay)

        except APITimeoutError as e:
            # íƒ€ì„ì•„ì›ƒì€ ë” ê¸´ ëŒ€ê¸°
            delay = min(base_delay * (2 ** attempt), max_delay * 2)
            await asyncio.sleep(delay)

        except Exception as e:
            # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” ì¦‰ì‹œ ì‹¤íŒ¨
            raise e
```

### ì¬ì‹œë„ ì „ëµ ë¹„êµ

| ì „ëµ | ëŒ€ê¸° ì‹œê°„ ê³µì‹ | ì¥ì  | ë‹¨ì  | ì¶”ì²œ ìƒí™© |
|------|---------------|------|------|-----------|
| `exponential_backoff` | 2^attempt | ë¹ ë¥¸ íšŒë³µ | ì„œë²„ ë¶€í•˜ ê°€ëŠ¥ | ì¼ë°˜ì  |
| `incremental` | attempt Ã— base | ì˜ˆì¸¡ ê°€ëŠ¥ | ëŠë¦° íšŒë³µ | ì•ˆì •ì ì¸ ì„œë¹„ìŠ¤ |
| `native_retry` | ì œê³µì ì˜ì¡´ | ìµœì í™”ë¨ | ì œê³µìë§ˆë‹¤ ë‹¤ë¦„ | ë‹¨ì¼ ì œê³µì |
| `random` | ë¬´ì‘ìœ„ | ì¶©ëŒ ë°©ì§€ | ëŠë¦° íšŒë³µ | ë¶„ì‚° ì‹œìŠ¤í…œ |

## ğŸ“Š í† í° ê´€ë¦¬

### í† í° ì¹´ìš´í„° êµ¬ì¡°

```mermaid
flowchart TB
    A[í…ìŠ¤íŠ¸] --> B{ëª¨ë¸ ì‹ë³„}

    B --> C["cl100k_base<br/>GPT-4, 3.5"]
    B --> D["o200k_base<br/>GPT-4o"]
    B --> E["p50k_base<br/>Code"]

    C --> F["tiktoken ì¸ì½”ë”"]
    D --> F
    E --> F

    F --> G["í† í° ìˆ˜ ë°˜í™˜"]

    style A fill:#e1f5fe
    style F fill:#fff9c4
    style G fill:#c8e6c9
```

```python
class Tokenizer:
    # tiktoken ì¸ì½”ë”© ë§¤í•‘
    ENCODINGS = {
        "cl100k_base": {  # GPT-4, GPT-3.5-Turbo
            "models": ["gpt-4", "gpt-3.5-turbo", "text-embedding-3-*"]
        },
        "o200k_base": {  # GPT-4o
            "models": ["gpt-4o"]
        },
        "p50k_base": {  # Code models
            "models": ["code-davinci-*"]
        }
    }

    def __init__(self, model: str):
        self.encoding = self._get_encoding(model)

    def count(self, text: str) -> int:
        """í…ìŠ¤íŠ¸ì˜ í† í° ìˆ˜ ë°˜í™˜"""
        return len(self.encoding.encode(text))

    def truncate(self, text: str, max_tokens: int) -> str:
        """í† í° ì œí•œìœ¼ë¡œ í…ìŠ¤íŠ¸ ìë¥´ê¸°"""
        tokens = self.encoding.encode(text)
        truncated = tokens[:max_tokens]
        return self.encoding.decode(truncated)
```

### ëª¨ë¸ë³„ ì¸ì½”ë”©

```mermaid
flowchart LR
    subgraph Models["ëª¨ë¸ë³„ ì¸ì½”ë”©"]
        GPT4["GPT-4<br/>cl100k_base"]
        GPT35["GPT-3.5<br/>cl100k_base"]
        GPT4O["GPT-4o<br/>o200k_base"]
        CODE["Code<br/>p50k_base"]
    end

    style GPT4 fill:#c8e6c9
    style GPT35 fill:#a5d6a7
    style GPT4O fill:#fff9c4
    style CODE fill:#e1bee7
```

## ğŸ”§ ë¹„ë™ê¸° ì²˜ë¦¬

### ë³‘ë ¬ ìš”ì²­ ì•„í‚¤í…ì²˜

```mermaid
flowchart TB
    A["í”„ë¡¬í”„íŠ¸ ëª©ë¡<br/>100ê°œ"] --> B["ì„¸ë§ˆí¬ì–´<br/>concurrent=25"]

    B --> C["ë³‘ë ¬ ì‹¤í–‰ ì‹œì‘"]

    C --> D1["ìš”ì²­ 1-25"]
    C --> D2["ìš”ì²­ 26-50"]
    C --> D3["ìš”ì²­ 51-75"]
    C --> D4["ìš”ì²­ 76-100"]

    D1 --> E["ê²°ê²° ìˆ˜ì§‘"]
    D2 --> E
    D3 --> E
    D4 --> E

    E --> F["ê²°ê³¼ ë°˜í™˜"]

    style A fill:#e1f5fe
    style B fill:#ffcdd2
    style E fill:#c8e6c9
    style F fill:#a5d6a7
```

```python
async def parallel_llm_calls(
    prompts: list[str],
    llm: BaseLanguageModel,
    concurrent_requests: int = 25
) -> list[str]:
    """
    ì—¬ëŸ¬ í”„ë¡¬í”„íŠ¸ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬
    """
    # ì„¸ë§ˆí¬ì–´ë¡œ ë™ì‹œ ìš”ì²­ ìˆ˜ ì œí•œ
    semaphore = asyncio.Semaphore(concurrent_requests)

    async def process(prompt: str):
        async with semaphore:
            return await llm.execute(prompt)

    # ë³‘ë ¬ ì‹¤í–‰
    tasks = [process(p) for p in prompts]
    results = await asyncio.gather(*tasks, return_exceptions=True)

    # ì˜ˆì™¸ ì²˜ë¦¬
    final_results = []
    for result in results:
        if isinstance(result, Exception):
            logging.error(f"LLM call failed: {result}")
            final_results.append("")  # ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¬¸ìì—´
        else:
            final_results.append(result)

    return final_results
```

### Async ëª¨ë“œ ì„ íƒ

| ëª¨ë“œ | ì„¤ëª… | ì‚¬ìš© ì‚¬ë¡€ |
|------|------|----------|
| `asyncio` | ì§„ì •í•œ ë¹„ë™ê¸° I/O | I/O ë°”ìš´ë“œ ì‘ì—… |
| `threaded` | ìŠ¤ë ˆë“œ í’€ì—ì„œ ì‹¤í–‰ | CPU ë°”ìš´ë“œ ì‘ì—… |

## ğŸ“ ê³ ê¸‰ ê¸°ë²•

### ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ

```mermaid
flowchart LR
    A["ìš”ì²­"] --> B["ìŠ¤íŠ¸ë¦¼ ì‹œì‘"]

    B --> C1["ì²­í¬ 1"]
    B --> C2["ì²­í¬ 2"]
    B --> C3["ì²­í¬ N"]

    C1 --> D["ì‹¤ì‹œê°„ ì¶œë ¥"]
    C2 --> D
    C3 --> D

    style A fill:#e1f5fe
    style D fill:#c8e6c9
```

```python
async def stream_execute(
    self,
    prompt: str
) -> AsyncIterator[str]:
    """
    ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ìƒì„±
    """
    async with self.client.chat.completions.create(
        model=self.model,
        messages=[{"role": "user", "content": prompt}],
        stream=True
    ) as response:
        async for chunk in response:
            if chunk.choices[0].delta.content:
                yield chunk.choices[0].delta.content
```

### í•¨ìˆ˜ í˜¸ì¶œ (Function Calling)

```mermaid
flowchart TB
    A["í…ìŠ¤íŠ¸"] --> B["LLM + Tools"]

    B --> C{"í•¨ìˆ˜ í˜¸ì¶œ<br/>í•„ìš”?"}

    C -->|ì˜ˆ| D["Tool ì‹¤í–‰"]
    C -->|ì•„ë‹ˆì˜¤| E["ì§ì ‘ ì‘ë‹µ"]

    D --> F["ê²°ê³¼ í¬í•¨<br/>ìµœì¢… ì‘ë‹µ"]

    style A fill:#e1f5fe
    style D fill:#fff9c4
    style F fill:#c8e6c9
```

## ğŸ“Š ì„±ëŠ¥ ìµœì í™”

### ìµœì í™” ì „ëµ

```mermaid
mindmap
    root((ìµœì í™”))
        ë°°ì¹˜ ì²˜ë¦¬
            í•œ ë²ˆì˜ API í˜¸ì¶œ
            ì—¬ëŸ¬ ì²­í¬ ì²˜ë¦¬
        ìºì‹±
            í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
            LLM ì‘ë‹µ
        ë³‘ë ¬ ì²˜ë¦¬
            asyncio
            ì„¸ë§ˆí¬ì–´ ì œì–´
        í† í° ìµœì í™”
            max_tokens ì œì–´
            íš¨ìœ¨ì  í”„ë¡¬í”„íŠ¸
```

## ğŸ”— ê´€ë ¨ ì»´í¬ë„ŒíŠ¸

- [[Index Module]]: ì¸ë±ì‹± ì¤‘ LLM ì‚¬ìš©
- [[Query Module]]: ì¿¼ë¦¬ ì²˜ë¦¬ ì¤‘ LLM ì‚¬ìš©
- [[Storage Module]]: ì‘ë‹µ ìºì‹±

## ğŸ’¡ ë¹„ìš© ìµœì í™” íŒ

1. **ìºì‹± í™œì„±í™”**: ë™ì¼ ìš”ì²­ ì¬ì²˜ë¦¬ ë°©ì§€
2. **ë°°ì¹˜ ì²˜ë¦¬**: ì—¬ëŸ¬ ìš”ì²­ì„ í•œ ë²ˆì— ì²˜ë¦¬
3. **ëª¨ë¸ ì„ íƒ**: ì‘ì—…ì— ë§ëŠ” ëª¨ë¸ ì„ íƒ
4. **í† í° ì œí•œ**: `max_tokens`ë¡œ ì¶œë ¥ ê¸¸ì´ ì œì–´

---
*See also: [[Language Model Module]], [[Index Module]], [[Query Module]]*
